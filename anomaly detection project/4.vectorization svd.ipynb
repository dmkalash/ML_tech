{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соревнование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачивание текста страницы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28026it [00:16, 1714.90it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "\n",
    "for num, filename in tqdm(enumerate(os.listdir('compiled'))):\n",
    "    if filename == '.DS_Store':\n",
    "        continue\n",
    "    num = filename.split('.', 1)[0]\n",
    "    with open('compiled/' + filename) as f:\n",
    "        dt = f.read()\n",
    "        doc_to_title[int(num)] = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'тип: реферат; размер: . kb.; резюме: в статье систематизированы клинические и лабораторные данные, свидетельствующие об эндокринных расстройствах, приводящих к бесплодию, или сопутствующих ему. даны рекомендации по рационализации и минимизации числа лабораторных анализов на этапе обследования пациентов с бесплодием м. б. аншина центр репродукции и генетики «фертимед», г. москва документы рефераты сочинения гдз м. б. аншина центр репродукции и генетики «фертимед», г. москва\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_title[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Составление трейн-тест групп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    \n",
    "    title = doc_to_title[doc_id]\n",
    "    \n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "testgroups_titledata = {}\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    \n",
    "    title = doc_to_title[doc_id]\n",
    "    \n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "        \n",
    "    testgroups_titledata[doc_group].append((doc_id, title, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Dmitry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = []\n",
    "with open('stop words.txt') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip())\n",
    "stop_words[0] = stop_words[0][-3:]\n",
    "stop_words += ['http', 'ru', 'com']\n",
    "\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords.extend(['это', 'нею'])\n",
    "\n",
    "stop_words += russian_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD по группе, расстояния и статистики в группе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def get_tf_vect(docs):\n",
    "    texts = []\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        texts.append(title)\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "    vectorizer.fit(texts)\n",
    "    return vectorizer\n",
    "\n",
    "def get_tf_trans(model, text):\n",
    "    return model.transform([text]).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [get_tf_vect, get_tf_trans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [02:30<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 19) (11690,) (11690,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "groups_train = []\n",
    "\n",
    "for new_group in tqdm(traingroups_titledata):\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    model = funcs[0](docs)\n",
    "    \n",
    "    vectors = []\n",
    "    for k, (doc_id, text, target_id) in enumerate(docs):\n",
    "        vect_text = funcs[1](model, text) \n",
    "        vectors.append(vect_text)\n",
    "        \n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    vectors = svd.fit_transform(vectors)\n",
    "    \n",
    "    center = np.mean(vectors, axis=0)\n",
    "    \n",
    "    lrad = 0\n",
    "    rrad = 10000\n",
    "    eps = 0.0001\n",
    "    ncount = int(len(vectors) * 0.8)\n",
    "    \n",
    "    median = np.median(vectors)\n",
    "    \n",
    "    while rrad - lrad > eps:\n",
    "        mid = (lrad + rrad) / 2\n",
    "        cnt = 0\n",
    "        for vect in vectors:\n",
    "            dst = np.sum((median - vect) ** 2)\n",
    "            if dst < mid * mid:\n",
    "                cnt += 1\n",
    "        if cnt <= ncount:\n",
    "            lrad = mid\n",
    "        else:\n",
    "            rrad = mid\n",
    "    \n",
    "    for k, (doc_id, text, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        \n",
    "        vect_text = vectors[k]\n",
    "        \n",
    "        cnt = 0\n",
    "        \n",
    "        dist = []\n",
    "        for j, (j_doc_id, j_text, j_target_id) in enumerate(docs):\n",
    "            if j == k:\n",
    "                continue\n",
    "            j_vect_text = vectors[j]\n",
    "            dist.append(cosine(vect_text, j_vect_text))\n",
    "            \n",
    "            dst = np.sum((vectors[k] - vectors[j]) ** 2)\n",
    "            if dst < rrad * rrad:\n",
    "                cnt += 1\n",
    "        \n",
    "        center_dist = cosine(center, vect_text)\n",
    "        mean_sq_dist = np.mean(np.array(dist) ** 2)\n",
    "        dist = np.sort(dist)[:15]\n",
    "        \n",
    "        sq_mean5 = np.mean(dist[:5] ** 2)\n",
    "        \n",
    "        X_train.append( np.concatenate(( dist, np.array([center_dist, mean_sq_dist, cnt, sq_mean5]) )) )\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "\n",
    "print(X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26155087, 0.28023325, 0.63909363, 0.71518123, 0.72712701,\n",
       "        0.77126297, 0.78902297, 0.80523784, 0.82530073, 0.83086177,\n",
       "        0.83689377, 0.83719606, 0.8375959 , 0.84952542, 0.85811398,\n",
       "        0.58131501, 0.87830123, 2.        , 0.31911562]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 67/180 [01:21<02:03,  1.09s/it]/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/spatial/distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "100%|██████████| 180/180 [03:18<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "groups_test = []\n",
    "\n",
    "for new_group in tqdm(testgroups_titledata):\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    model = funcs[0](docs) \n",
    "    \n",
    "    vectors = []\n",
    "    for k, (doc_id, text, _) in enumerate(docs):\n",
    "        vect_text = funcs[1](model, text) \n",
    "        vectors.append(vect_text)\n",
    "        \n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    vectors = svd.fit_transform(vectors)\n",
    "    \n",
    "    center = np.mean(vectors, axis=0)\n",
    "    \n",
    "    lrad = 0\n",
    "    rrad = 10000\n",
    "    eps = 0.0001\n",
    "    ncount = int(len(vectors) * 0.8)\n",
    "    \n",
    "    median = np.median(vectors)\n",
    "    \n",
    "    while rrad - lrad > eps:\n",
    "        mid = (lrad + rrad) / 2\n",
    "        cnt = 0\n",
    "        for vect in vectors:\n",
    "            dst = np.sum((median - vect) ** 2)\n",
    "            if dst < mid * mid:\n",
    "                cnt += 1\n",
    "        if cnt <= ncount:\n",
    "            lrad = mid\n",
    "        else:\n",
    "            rrad = mid\n",
    "    \n",
    "    for k, (doc_id, text, _) in enumerate(docs):\n",
    "        groups_test.append(new_group)\n",
    "        \n",
    "        vect_text = vectors[k]\n",
    "\n",
    "        cnt = 0\n",
    "        \n",
    "        dist = []\n",
    "        \n",
    "        for j, (j_doc_id, j_text, _) in enumerate(docs):\n",
    "            if j == k:\n",
    "                continue\n",
    "            j_vect_text = vectors[j]\n",
    "            dist.append(cosine(vect_text, j_vect_text))\n",
    "            \n",
    "            dst = np.sum((vectors[k] - vectors[j]) ** 2)\n",
    "            if dst < rrad * rrad:\n",
    "                cnt += 1\n",
    "        \n",
    "        center_dist = cosine(center, vect_text)\n",
    "        mean_sq_dist = np.mean(np.array(dist) ** 2)\n",
    "        dist = np.sort(dist)[:15]\n",
    "        \n",
    "        sq_mean5 = np.mean(dist[:5] ** 2)\n",
    "        \n",
    "        X_test.append( np.concatenate(( dist, np.array([center_dist, mean_sq_dist, cnt, sq_mean5]) )) )\n",
    "        \n",
    "X_test = np.array(X_test)\n",
    "groups_test = np.array(groups_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 19)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56311887, 0.57032456, 0.5871184 , 0.61859738, 0.652734  ,\n",
       "        0.66479215, 0.67216785, 0.72397733, 0.7455059 , 0.74803881,\n",
       "        0.74967039, 0.80758432, 0.81902166, 0.84141607, 0.84660421,\n",
       "        0.52691717, 0.87988677, 0.        , 0.35916107]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[np.isnan(X_train)] = 1\n",
    "X_test[np.isnan(X_test)] = 1\n",
    "\n",
    "np.save('x_train24.npy', X_train)\n",
    "np.save('y_train24.npy', y_train)\n",
    "np.save('x_test24.npy', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
